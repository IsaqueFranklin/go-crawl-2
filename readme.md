# ğŸ“ Estrutura do Projeto de Crawler em Go

## ğŸ”§ FunÃ§Ãµes / Arquivos

- **`main`**:  
  Ponto de entrada do programa, onde tudo Ã© inicializado.

- **`crawl`**:  
  FunÃ§Ã£o principal responsÃ¡vel por:
  - Fazer a requisiÃ§Ã£o HTTP
  - Fazer o parse do HTML
  - Extrair os links da pÃ¡gina

- **`worker`**:  
  FunÃ§Ã£o executada por cada goroutine para processar as URLs da fila (concorrÃªncia).

- **`addToQueue`**:  
  Adiciona uma nova URL Ã  fila de URLs a serem visitadas, evitando duplicatas.

- **`saveURLs`**:  
  Salva as URLs coletadas em um arquivo de saÃ­da.

---

## ğŸ“š Bibliotecas Utilizadas

- **`colly`**:  
  Usada para simplificar a lÃ³gica de crawling. Essa biblioteca jÃ¡ cuida de muitas complexidades como:
  - Gerenciamento de links visitados
  - ExecuÃ§Ã£o concorrente
  - RequisiÃ§Ãµes HTTP
  - Parse de pÃ¡ginas

- **`goquery`**:  
  Embora o `colly` tenha seu prÃ³prio parser, o `goquery` pode ser Ãºtil para parsing HTML mais complexo.  
  Ele permite manipular o DOM de forma similar ao jQuery.

---

> âœ… Com essa estrutura, o projeto se mantÃ©m organizado, modular e fÃ¡cil de expandir.
